<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Mining | Yu Cao</title>
    <link>https://yucao16.com/category/data-mining/</link>
      <atom:link href="https://yucao16.com/category/data-mining/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Mining</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Yu Cao 2021</copyright><lastBuildDate>Mon, 02 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yucao16.com/img/sharon-mccutcheon-62vi3TG5EDg-unsplash_small.jpg</url>
      <title>Data Mining</title>
      <link>https://yucao16.com/category/data-mining/</link>
    </image>
    
    <item>
      <title>Comparison of data dimensionality reduction methods!</title>
      <link>https://yucao16.com/blog/pca-and-umap/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/blog/pca-and-umap/</guid>
      <description>
&lt;script src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this blog, I mainly compared two dimensionality reduction methods, i.e. PCA (Principle Component Analysis) and UMAP (Uniform manifold approximation and projection). Let’s explore which one is better, PCA, the traditional method of dimensionality reduction or, UMAP the new boy.&lt;/p&gt;
&lt;p&gt;Let’s start with data mining. If you want to remember the codes, then you need to practice them everyday, right?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
theme_set(theme_light())
library(janitor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset used in this blog comes from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;TidyTuesday Dataset&lt;/a&gt;, 2020-05-26, about cocktail recipes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boston_cocktails &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv&amp;quot;)

boston_cocktails %&amp;gt;% 
  count(ingredient, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 569 × 2
##    ingredient            n
##    &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 Gin                 176
##  2 Fresh lemon juice   138
##  3 Simple Syrup        115
##  4 Vodka               114
##  5 Light Rum           113
##  6 Dry Vermouth        107
##  7 Fresh Lime Juice    107
##  8 Triple Sec          107
##  9 Powdered Sugar       90
## 10 Grenadine            85
## # … with 559 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there are no missing values in this dataset, the ‘ingredient’ column, for example, contains many duplicate labes, such as Juice of a Lime and Lime Juice. So I did a series of data cleaning, which involved sorting out the duplicate labels in column ‘ingredient’ and, converting the categorical variables in the ‘measure’ column to numerical varables, and here is the codes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_parsed &amp;lt;- boston_cocktails %&amp;gt;%
  filter(!measure == &amp;quot;splash&amp;quot;) %&amp;gt;% 
  mutate(
    ingredient = str_to_lower(ingredient),
    ingredient = str_replace_all(ingredient, &amp;quot;-&amp;quot;, &amp;quot; &amp;quot;),
    ingredient = str_remove(ingredient, &amp;quot; liqueur$&amp;quot;),
    ingredient = str_remove(ingredient, &amp;quot; (if desired)$&amp;quot;),
    ingredient = case_when(
      str_detect(ingredient, &amp;quot;bitters&amp;quot;) ~ &amp;quot;bitters&amp;quot;,
      str_detect(ingredient, &amp;quot;lemon&amp;quot;) ~ &amp;quot;lemon juice&amp;quot;,
      str_detect(ingredient, &amp;quot;lime&amp;quot;) ~ &amp;quot;lime juice&amp;quot;,
      str_detect(ingredient, &amp;quot;grapefruit&amp;quot;) ~ &amp;quot;grapefruit juice&amp;quot;,
      str_detect(ingredient, &amp;quot;orange&amp;quot;) ~ &amp;quot;orange juice&amp;quot;,
      TRUE ~ ingredient
    ),
    measure = str_replace(measure, &amp;quot; ?1/2&amp;quot;, &amp;quot;.5&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?3/4&amp;quot;, &amp;quot;.75&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?1/4&amp;quot;, &amp;quot;.25&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?For glass&amp;quot;, &amp;quot;12&amp;quot;),
    measure_number = parse_number(measure)) %&amp;gt;% 
  add_count(ingredient) %&amp;gt;%
  filter(n &amp;gt; 15) %&amp;gt;%
  select(-n) %&amp;gt;%
  distinct(row_id, ingredient, .keep_all = TRUE) %&amp;gt;%
  na.omit() %&amp;gt;% 
  select(-ingredient_number, -row_id, -measure)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it looks much better, doesn’t it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_parsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,561 × 4
##    name                 category          ingredient     measure_number
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt;
##  1 Gauguin              Cocktail Classics light rum                2   
##  2 Gauguin              Cocktail Classics lemon juice              1   
##  3 Gauguin              Cocktail Classics lime juice               1   
##  4 Fort Lauderdale      Cocktail Classics light rum                1.5 
##  5 Fort Lauderdale      Cocktail Classics sweet vermouth           0.5 
##  6 Fort Lauderdale      Cocktail Classics orange juice             0.25
##  7 Fort Lauderdale      Cocktail Classics lime juice               0.25
##  8 Cuban Cocktail No. 1 Cocktail Classics lime juice               0.5 
##  9 Cuban Cocktail No. 1 Cocktail Classics powdered sugar           0.5 
## 10 Cuban Cocktail No. 1 Cocktail Classics light rum                2   
## # … with 2,551 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s time for modeling, in order to apply dimensionality reduction method, you should at least have a high dimension dataset, right? What I gonna do is make the dataset wider by using &lt;code&gt;pivot_wider()&lt;/code&gt; from package &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_df &amp;lt;- cocktails_parsed %&amp;gt;% 
  pivot_wider(names_from = ingredient, values_from = measure_number, values_fill = 0) %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  na.omit()

cocktails_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 937 × 42
##    name    category light_rum lemon_juice lime_juice sweet_vermouth orange_juice
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Gauguin Cocktai…      2           1          1               0           0   
##  2 Fort L… Cocktai…      1.5         0          0.25            0.5         0.25
##  3 Cuban … Cocktai…      2           0          0.5             0           0   
##  4 Cool C… Cocktai…      0           0          0               0           1   
##  5 John C… Whiskies      0           1          0               0           0   
##  6 Cherry… Cocktai…      1.25        0          0               0           0   
##  7 Casa B… Cocktai…      2           0          1.5             0           0   
##  8 Caribb… Cocktai…      0.5         0          0               0           0   
##  9 Amber … Cordial…      0           0.25       0               0           0   
## 10 The Jo… Whiskies      0           0.5        0               0           0   
## # … with 927 more rows, and 35 more variables: powdered_sugar &amp;lt;dbl&amp;gt;,
## #   dark_rum &amp;lt;dbl&amp;gt;, cranberry_juice &amp;lt;dbl&amp;gt;, pineapple_juice &amp;lt;dbl&amp;gt;,
## #   bourbon_whiskey &amp;lt;dbl&amp;gt;, simple_syrup &amp;lt;dbl&amp;gt;, cherry_flavored_brandy &amp;lt;dbl&amp;gt;,
## #   light_cream &amp;lt;dbl&amp;gt;, triple_sec &amp;lt;dbl&amp;gt;, maraschino &amp;lt;dbl&amp;gt;, amaretto &amp;lt;dbl&amp;gt;,
## #   grenadine &amp;lt;dbl&amp;gt;, apple_brandy &amp;lt;dbl&amp;gt;, brandy &amp;lt;dbl&amp;gt;, gin &amp;lt;dbl&amp;gt;,
## #   anisette &amp;lt;dbl&amp;gt;, dry_vermouth &amp;lt;dbl&amp;gt;, apricot_flavored_brandy &amp;lt;dbl&amp;gt;,
## #   bitters &amp;lt;dbl&amp;gt;, straight_rye_whiskey &amp;lt;dbl&amp;gt;, benedictine &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we go, we got a dataset with 42 columns, which is a series problem if you gonna do some classification word on this dataset. In the following, I will demonstrate the two dimensionality reduction method separately.&lt;/p&gt;
&lt;div id=&#34;principle-component-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Principle Component Analysis&lt;/h3&gt;
&lt;p&gt;One of the benefits of the &lt;code&gt;tidymodels&lt;/code&gt; ecosystem is the flexibility and ease of trying different approaches for the same kind of task. With function &lt;code&gt;recipe&lt;/code&gt;, you don’t have to perform every step, you only need to write them down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

pca_rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
  update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
  step_normalize(all_predictors()) %&amp;gt;% 
  step_pca(all_predictors(), num_comp = 2)

pca_prep &amp;lt;- prep(pca_rec)

pca_prep&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, let’s focus on the variable &lt;code&gt;pca_rec&lt;/code&gt;, what we have done is:&lt;br /&gt;
- First, tell &lt;code&gt;recipe()&lt;/code&gt; what’s going on with our model and what data we are using.
- Second, we update the role of &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;category&lt;/code&gt; columns by function &lt;code&gt;update_role()&lt;/code&gt;, since these are variables we want to keep around as identifiers for rows.
- Then we need to center and scale the numeric predictors.
- Finally, we use &lt;code&gt;step_pca&lt;/code&gt; for the actual principal component analysis. Note that actually, the PCA step is the second step. The &lt;code&gt;prep&lt;/code&gt; estimate the required parameters from a training set that can be later applied to other datasets for us.&lt;/p&gt;
&lt;p&gt;Once we have our recipe done, we can explore the results by using &lt;code&gt;tidy()&lt;/code&gt;, which makes it possible to visualize what each components look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidied_pca &amp;lt;- tidy(pca_prep, 2)

tidied_pca %&amp;gt;% filter(component %in% paste0(&amp;quot;PC&amp;quot;, 1:5)) %&amp;gt;% 
  mutate(component = fct_inorder(component)) %&amp;gt;% 
  ggplot(aes(value, terms, fill = terms)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~component, nrow = 1) + 
  labs(y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, we can see how much every component contributes to each &lt;code&gt;category&lt;/code&gt;. Clearly, the biggest difference in PC1 is powdered sugar vs. simple syrup. Let’s zoom in on the first four components, and understand which cocktail ingredients contribute in the positive and negative directions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)

tidied_pca %&amp;gt;% 
  filter(component %in% paste0(&amp;quot;PC&amp;quot;, 1:4)) %&amp;gt;% 
  group_by(component) %&amp;gt;% 
  top_n(8, abs(value)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(terms = reorder_within(terms, abs(value), component)) %&amp;gt;% 
  ggplot(aes(abs(value), terms, fill = value &amp;gt; 0)) + 
  geom_col() + 
  scale_y_reordered() +
  facet_wrap(~component, scales = &amp;quot;free_y&amp;quot;) + 
  labs(fill = &amp;quot;positive?&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So PC1 is about powdered sugar + egg + gin drinks vs. simple syrup + lime + tequila drinks. This is the component that explains the most variation in drinks. PC2 is mostly about vermouth, both sweet and dry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bake(pca_prep, new_data = NULL) %&amp;gt;%
  ggplot(aes(PC1, PC2, label = name)) +
  geom_point(aes(color = category), alpha = 0.7, size = 2) +
  geom_text(check_overlap = TRUE) +
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows how the cocktails distributed in the plane of the first two components. The &lt;code&gt;bake()&lt;/code&gt; return the computations accroding to the &lt;code&gt;recipe&lt;/code&gt;. We can conclude:
- Fizzy, egg, poweder sugar drinks are to the left.
- Simple syrup, lime, tequila drinks are to the right.
- Vermouth drinks are more to the top. Although many labels are mixed together, they may be distinguishable on other components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uniform-manifold-approximation-and-projection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Uniform manifold approximation and projection&lt;/h3&gt;
&lt;p&gt;Thanks to the benefits of the &lt;code&gt;tidymodels&lt;/code&gt; ecosystem, we can switch out PCA for UMAP, simply by replace &lt;code&gt;step_pca()&lt;/code&gt; by &lt;code&gt;step_umap&lt;/code&gt;. Before that, don’t forget to import package &lt;code&gt;embed&lt;/code&gt;, which provide recipe steps for ways to create embeddings including UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(embed)

umap_rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
  update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
  step_normalize(all_predictors()) %&amp;gt;% 
  step_umap(all_predictors(), num_comp = 5)

umap_prep &amp;lt;- prep(umap_rec)

umap_prep&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, we can see how the cocktails are distributed in the plane of the first two UMAP components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bake(umap_prep, new_data = NULL) %&amp;gt;%
  ggplot(aes(umap_1, umap_2, label = name)) +
  geom_point(aes(color = category), alpha = 0.7, size = 2) +
  geom_text(check_overlap = TRUE) +
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that the distribution of the first two components of UMAP is totally different from PCA. Obviously, most of the categories didn’t mix together. That’s what I’m talking about. And I can tell, the performance of the classification algorithm trained on the first two components of UMAP will definitely better than of PCA. Let’s prove it on the kernel SVM (Support Vector Machine).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)

svm_test &amp;lt;- function(num_compo, meth){
  if(meth == &amp;quot;PCA&amp;quot;){
    rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
    update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
    step_normalize(all_predictors()) %&amp;gt;% 
    step_pca(all_predictors(), num_comp = num_compo)
  }
  
  if(meth == &amp;quot;UMAP&amp;quot;){
    rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
    update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
    step_normalize(all_predictors()) %&amp;gt;% 
    step_umap(all_predictors(), num_comp = num_compo)
  }
  
  df &amp;lt;- bake(prep(rec), new_data = NULL)
  
  n &amp;lt;- nrow(df)
  ntrain &amp;lt;- round(n * 0.75)
  set.seed(123)
  
  tindex &amp;lt;- sample(n, ntrain)
  train_iris &amp;lt;- df[tindex, ]
  test_iris &amp;lt;- df[-tindex, ]

  svm1 &amp;lt;- svm(category~., data = train_iris, method = &amp;quot;C-classification&amp;quot;, 
              kernal=&amp;quot;radial&amp;quot;, gamma = 0.1, cost = 10)
  
  prediction &amp;lt;- predict(svm1, test_iris)

  xtab &amp;lt;- table(test_iris$category, prediction)
  result &amp;lt;- data.frame(test_iris$category, prediction)
  accuracy &amp;lt;- sum(test_iris$category == prediction) / nrow(test_iris)
  return(accuracy)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a function require two variables input, the number of component and demensionality reduction method, and output the accuracy of the classification. Let’s first verify the performance of the first two components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;PCA: &amp;quot;,svm_test(2, &amp;quot;PCA&amp;quot;), &amp;quot;, UMAP: &amp;quot;,svm_test(2,&amp;quot;UMAP&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PCA:  0.6410256 , UMAP:  0.7051282&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, the first two components of UMAP contains more classification information, at least make the SVM classify more accurate.&lt;/p&gt;
&lt;p&gt;You think that’s the end of it? We know that, in real life, we should always consider the time cost, and that is the meaningful of the dimensionality reduction method. So let add the time cost into the comparision, start by finding out how many components PCA needs to achieve the same accuracy as UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result_pca &amp;lt;- 1:10 %&amp;gt;% map_dbl(svm_test, meth = &amp;quot;PCA&amp;quot;) 
result_umap &amp;lt;- 1:10 %&amp;gt;% map_dbl(svm_test, meth = &amp;quot;UMAP&amp;quot;) 
result &amp;lt;- data.frame(pca = result_pca,
                     umap = result_umap) %&amp;gt;% stack()

result$index &amp;lt;- c(1:10,1:10)

result  %&amp;gt;% 
  ggplot(aes(index, values, color = ind)) + 
  geom_line() + 
  labs(color = &amp;quot;method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Seems like the accuracy converge at 75% as the number of components increase, actually after 5 components, PCA performs better than UMAP. Basically, PCA needs at least 5 components to get the height of UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
time_UMAP &amp;lt;- microbenchmark(svm_test(3,&amp;quot;UMAP&amp;quot;), times = 10, unit=&amp;quot;s&amp;quot;)
time_PCA &amp;lt;- microbenchmark(svm_test(5,&amp;quot;PCA&amp;quot;), times = 10, unit=&amp;quot;s&amp;quot;)
cat(&amp;quot;UMAP: &amp;quot;, mean(time_UMAP$time)/1e9, &amp;quot;, PCA: &amp;quot;, mean(time_PCA$time/1e9))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## UMAP:  2.005904 , PCA:  0.330938&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the result, PCA undoubtedly crushes UMAP. I know, I know, you mean that this is because the dataset is so small that the advantages of low dimensionality are not fully realezed, and this is just one example and doesn’t tell us much. I totally agree, I really am. &lt;strong&gt;What I would like to say is, the job of a data analyst should not be to blindly pursue new technologies, but to be flexible and efficient in getting tasks done.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the code for the data mining section is taken from &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;here&lt;/a&gt;, I’m very grateful to &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;Julia Silge&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;David Robinson&lt;/a&gt; for their videos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leave a message if you like this blog.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
