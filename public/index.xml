<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yu Cao</title>
    <link>https://yucao16.com/</link>
      <atom:link href="https://yucao16.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Yu Cao</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Yu Cao 2021</copyright><lastBuildDate>Mon, 25 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://yucao16.com/img/sharon-mccutcheon-62vi3TG5EDg-unsplash_small.jpg</url>
      <title>Yu Cao</title>
      <link>https://yucao16.com/</link>
    </image>
    
    <item>
      <title>Test</title>
      <link>https://yucao16.com/blog/test/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/blog/test/</guid>
      <description>
&lt;script src=&#34;https://yucao16.com/blog/test/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;a href=&#34;https://yucao16.com/.netlify/functions/hello&#34;&gt;say hello!&lt;/a&gt;&lt;/p&gt;
&lt;script src=&#34;https://utteranc.es/client.js&#34;
        repo=&#34;[ENTER REPO HERE]&#34;
        issue-term=&#34;pathname&#34; 
        theme=&#34;github-light&#34;
        crossorigin=&#34;anonymous&#34;
        async&gt;
&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>R Plot Gallery</title>
      <link>https://yucao16.com/blog/test-hide-code/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/blog/test-hide-code/</guid>
      <description>
&lt;script src=&#34;https://yucao16.com/blog/test-hide-code/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;script&gt;
function myFunction(x) {
  if (x.style.display === &#34;none&#34;) {
    x.style.display = &#34;block&#34;;
  } else {
    x.style.display = &#34;none&#34;;
  }
}
&lt;/script&gt;
&lt;style&gt;
button{
    background-color: white;
    color: #f44336;
    border-radius:12px;
    outline: 0;
    transition-duration: 0.4s;
    border-color:#f44336;
}

button:hover{
    background-color: #f44336;
    color: white;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Don’t be fooled by the subtitle that tells you this article is a 1 min read.&lt;/p&gt;
&lt;p&gt;In this post, I have listed plenty of plots generated by R. You can enjoy these plots as much as you like. If you are interested in how to reproduce these plots, please click on the &lt;code&gt;Code&lt;/code&gt; button below the plots. If you are interested in what these plots can tell you, please click on the &lt;code&gt;Explanation&lt;/code&gt; button next to &lt;code&gt;Code&lt;/code&gt;. I hope you enjoy this gallery!&lt;/p&gt;
&lt;figure style=&#34;padding: 0rem; margin: 0rem 0rem&#34;&gt;
&lt;img src=&#34;https://yucao16.com/blog/test-hide-code/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
&lt;button onclick=&#34;myFunction(myDIV1)&#34;&gt;Code&lt;/button&gt;
&lt;div id=&#34;myDIV1&#34; style=&#34;display: none&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins %&amp;gt;%
  filter(ott &amp;gt; 20, ott &amp;lt; 1e3) %&amp;gt;%
  ggplot(aes(ott, weight_lbs, color = place)) +
  geom_point(alpha = 0.2, size = 1.1) +
  labs(x = &amp;quot;over-the-top inches&amp;quot;, y = &amp;quot;weight (lbs)&amp;quot;) +
  scale_color_viridis_c()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;figure style=&#34;padding: 0rem; margin: 0rem 0rem&#34;&gt;
&lt;img src=&#34;https://yucao16.com/blog/test-hide-code/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
&lt;button onclick=&#34;myFunction(myDIV2)&#34;&gt;Code&lt;/button&gt;
&lt;div id=&#34;myDIV2&#34; style=&#34;display: none&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins %&amp;gt;%
  filter(ott &amp;gt; 20, ott &amp;lt; 1e3) %&amp;gt;%
  ggplot(aes(ott, weight_lbs)) +
  geom_point(alpha = 0.2, size = 1.1, color = &amp;quot;gray60&amp;quot;) +
  geom_smooth(aes(color = factor(year)),
    method = lm, formula = y ~ splines::bs(x, 3),
    se = FALSE, size = 1.5, alpha = 0.6
  ) +
  labs(x = &amp;quot;over-the-top inches&amp;quot;, y = &amp;quot;weight (lbs)&amp;quot;, color = NULL) +
  scale_color_viridis_d()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;figure style=&#34;padding: 0rem; margin: 0rem 0rem&#34;&gt;
&lt;img src=&#34;https://yucao16.com/blog/test-hide-code/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
&lt;button onclick=&#34;myFunction(myDIV3)&#34;&gt;Code&lt;/button&gt;
&lt;div id=&#34;myDIV3&#34; style=&#34;display: none&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pumpkins %&amp;gt;%
  mutate(
    country = fct_lump(country, n = 10),
    country = fct_reorder(country, weight_lbs)
  ) %&amp;gt;%
  ggplot(aes(country, weight_lbs, color = country)) +
  geom_boxplot(outlier.colour = NA) +
  geom_jitter(alpha = 0.1, width = 0.15) +
  labs(x = NULL, y = &amp;quot;weight (lbs)&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;button onclick=&#34;myFunction(explain3)&#34;&gt;
Explanation
&lt;/button&gt;
&lt;div id=&#34;explain3&#34; style=&#34;display: none&#34;&gt;
&lt;p&gt;dafsafdafdafda&lt;/p&gt;
&lt;/div&gt;
&lt;/figure&gt;
&lt;figure style=&#34;padding: 0rem; margin: 0rem 0rem&#34;&gt;
&lt;img src=&#34;https://yucao16.com/blog/test-hide-code/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
&lt;button onclick=&#34;myFunction(AfH2K7Wc0h)&#34;&gt;Code&lt;/button&gt;
&lt;div id=&#34;AfH2K7Wc0h&#34; style=&#34;display: none&#34;&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;water_raw %&amp;gt;%
  filter(
    country_name == &amp;quot;Sierra Leone&amp;quot;,
    lat_deg &amp;gt; 0, lat_deg &amp;lt; 15, lon_deg &amp;lt; 0,
    status_id %in% c(&amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;)
  ) %&amp;gt;%
  ggplot(aes(lon_deg, lat_deg, color = status_id)) +
  geom_point(alpha = 0.1) +
  coord_fixed() +
  guides(color = guide_legend(override.aes = list(alpha = 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;button onclick=&#34;myFunction(kaDimFIfN7)&#34;&gt;
Explanation
&lt;/button&gt;
&lt;div id=&#34;kaDimFIfN7&#34; style=&#34;display: none&#34;&gt;

&lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;To be continued…&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparison of data dimensionality reduction methods!</title>
      <link>https://yucao16.com/blog/pca-and-umap/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/blog/pca-and-umap/</guid>
      <description>
&lt;script src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this blog, I mainly compared two dimensionality reduction methods, i.e. PCA (Principle Component Analysis) and UMAP (Uniform manifold approximation and projection). Let’s explore which one is better, PCA, the traditional method of dimensionality reduction or, UMAP the new boy.&lt;/p&gt;
&lt;p&gt;Let’s start with data mining. If you want to remember the codes, then you need to practice them everyday, right?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
theme_set(theme_light())
library(janitor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset used in this blog comes from &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;TidyTuesday Dataset&lt;/a&gt;, 2020-05-26, about cocktail recipes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boston_cocktails &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv&amp;quot;)

boston_cocktails %&amp;gt;% 
  count(ingredient, sort = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 569 × 2
##    ingredient            n
##    &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt;
##  1 Gin                 176
##  2 Fresh lemon juice   138
##  3 Simple Syrup        115
##  4 Vodka               114
##  5 Light Rum           113
##  6 Dry Vermouth        107
##  7 Fresh Lime Juice    107
##  8 Triple Sec          107
##  9 Powdered Sugar       90
## 10 Grenadine            85
## # … with 559 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although there are no missing values in this dataset, the ‘ingredient’ column, for example, contains many duplicate labes, such as Juice of a Lime and Lime Juice. So I did a series of data cleaning, which involved sorting out the duplicate labels in column ‘ingredient’ and, converting the categorical variables in the ‘measure’ column to numerical varables, and here is the codes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_parsed &amp;lt;- boston_cocktails %&amp;gt;%
  filter(!measure == &amp;quot;splash&amp;quot;) %&amp;gt;% 
  mutate(
    ingredient = str_to_lower(ingredient),
    ingredient = str_replace_all(ingredient, &amp;quot;-&amp;quot;, &amp;quot; &amp;quot;),
    ingredient = str_remove(ingredient, &amp;quot; liqueur$&amp;quot;),
    ingredient = str_remove(ingredient, &amp;quot; (if desired)$&amp;quot;),
    ingredient = case_when(
      str_detect(ingredient, &amp;quot;bitters&amp;quot;) ~ &amp;quot;bitters&amp;quot;,
      str_detect(ingredient, &amp;quot;lemon&amp;quot;) ~ &amp;quot;lemon juice&amp;quot;,
      str_detect(ingredient, &amp;quot;lime&amp;quot;) ~ &amp;quot;lime juice&amp;quot;,
      str_detect(ingredient, &amp;quot;grapefruit&amp;quot;) ~ &amp;quot;grapefruit juice&amp;quot;,
      str_detect(ingredient, &amp;quot;orange&amp;quot;) ~ &amp;quot;orange juice&amp;quot;,
      TRUE ~ ingredient
    ),
    measure = str_replace(measure, &amp;quot; ?1/2&amp;quot;, &amp;quot;.5&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?3/4&amp;quot;, &amp;quot;.75&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?1/4&amp;quot;, &amp;quot;.25&amp;quot;),
    measure = str_replace(measure, &amp;quot; ?For glass&amp;quot;, &amp;quot;12&amp;quot;),
    measure_number = parse_number(measure)) %&amp;gt;% 
  add_count(ingredient) %&amp;gt;%
  filter(n &amp;gt; 15) %&amp;gt;%
  select(-n) %&amp;gt;%
  distinct(row_id, ingredient, .keep_all = TRUE) %&amp;gt;%
  na.omit() %&amp;gt;% 
  select(-ingredient_number, -row_id, -measure)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it looks much better, doesn’t it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_parsed&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,561 × 4
##    name                 category          ingredient     measure_number
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt;
##  1 Gauguin              Cocktail Classics light rum                2   
##  2 Gauguin              Cocktail Classics lemon juice              1   
##  3 Gauguin              Cocktail Classics lime juice               1   
##  4 Fort Lauderdale      Cocktail Classics light rum                1.5 
##  5 Fort Lauderdale      Cocktail Classics sweet vermouth           0.5 
##  6 Fort Lauderdale      Cocktail Classics orange juice             0.25
##  7 Fort Lauderdale      Cocktail Classics lime juice               0.25
##  8 Cuban Cocktail No. 1 Cocktail Classics lime juice               0.5 
##  9 Cuban Cocktail No. 1 Cocktail Classics powdered sugar           0.5 
## 10 Cuban Cocktail No. 1 Cocktail Classics light rum                2   
## # … with 2,551 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s time for modeling, in order to apply dimensionality reduction method, you should at least have a high dimension dataset, right? What I gonna do is make the dataset wider by using &lt;code&gt;pivot_wider()&lt;/code&gt; from package &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cocktails_df &amp;lt;- cocktails_parsed %&amp;gt;% 
  pivot_wider(names_from = ingredient, values_from = measure_number, values_fill = 0) %&amp;gt;% 
  janitor::clean_names() %&amp;gt;% 
  na.omit()

cocktails_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 937 × 42
##    name    category light_rum lemon_juice lime_juice sweet_vermouth orange_juice
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 Gauguin Cocktai…      2           1          1               0           0   
##  2 Fort L… Cocktai…      1.5         0          0.25            0.5         0.25
##  3 Cuban … Cocktai…      2           0          0.5             0           0   
##  4 Cool C… Cocktai…      0           0          0               0           1   
##  5 John C… Whiskies      0           1          0               0           0   
##  6 Cherry… Cocktai…      1.25        0          0               0           0   
##  7 Casa B… Cocktai…      2           0          1.5             0           0   
##  8 Caribb… Cocktai…      0.5         0          0               0           0   
##  9 Amber … Cordial…      0           0.25       0               0           0   
## 10 The Jo… Whiskies      0           0.5        0               0           0   
## # … with 927 more rows, and 35 more variables: powdered_sugar &amp;lt;dbl&amp;gt;,
## #   dark_rum &amp;lt;dbl&amp;gt;, cranberry_juice &amp;lt;dbl&amp;gt;, pineapple_juice &amp;lt;dbl&amp;gt;,
## #   bourbon_whiskey &amp;lt;dbl&amp;gt;, simple_syrup &amp;lt;dbl&amp;gt;, cherry_flavored_brandy &amp;lt;dbl&amp;gt;,
## #   light_cream &amp;lt;dbl&amp;gt;, triple_sec &amp;lt;dbl&amp;gt;, maraschino &amp;lt;dbl&amp;gt;, amaretto &amp;lt;dbl&amp;gt;,
## #   grenadine &amp;lt;dbl&amp;gt;, apple_brandy &amp;lt;dbl&amp;gt;, brandy &amp;lt;dbl&amp;gt;, gin &amp;lt;dbl&amp;gt;,
## #   anisette &amp;lt;dbl&amp;gt;, dry_vermouth &amp;lt;dbl&amp;gt;, apricot_flavored_brandy &amp;lt;dbl&amp;gt;,
## #   bitters &amp;lt;dbl&amp;gt;, straight_rye_whiskey &amp;lt;dbl&amp;gt;, benedictine &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we go, we got a dataset with 42 columns, which is a series problem if you gonna do some classification word on this dataset. In the following, I will demonstrate the two dimensionality reduction method separately.&lt;/p&gt;
&lt;div id=&#34;principle-component-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Principle Component Analysis&lt;/h3&gt;
&lt;p&gt;One of the benefits of the &lt;code&gt;tidymodels&lt;/code&gt; ecosystem is the flexibility and ease of trying different approaches for the same kind of task. With function &lt;code&gt;recipe&lt;/code&gt;, you don’t have to perform every step, you only need to write them down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)

pca_rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
  update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
  step_normalize(all_predictors()) %&amp;gt;% 
  step_pca(all_predictors(), num_comp = 2)

pca_prep &amp;lt;- prep(pca_rec)

pca_prep&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, let’s focus on the variable &lt;code&gt;pca_rec&lt;/code&gt;, what we have done is:&lt;br /&gt;
- First, tell &lt;code&gt;recipe()&lt;/code&gt; what’s going on with our model and what data we are using.
- Second, we update the role of &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;category&lt;/code&gt; columns by function &lt;code&gt;update_role()&lt;/code&gt;, since these are variables we want to keep around as identifiers for rows.
- Then we need to center and scale the numeric predictors.
- Finally, we use &lt;code&gt;step_pca&lt;/code&gt; for the actual principal component analysis. Note that actually, the PCA step is the second step. The &lt;code&gt;prep&lt;/code&gt; estimate the required parameters from a training set that can be later applied to other datasets for us.&lt;/p&gt;
&lt;p&gt;Once we have our recipe done, we can explore the results by using &lt;code&gt;tidy()&lt;/code&gt;, which makes it possible to visualize what each components look like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidied_pca &amp;lt;- tidy(pca_prep, 2)

tidied_pca %&amp;gt;% filter(component %in% paste0(&amp;quot;PC&amp;quot;, 1:5)) %&amp;gt;% 
  mutate(component = fct_inorder(component)) %&amp;gt;% 
  ggplot(aes(value, terms, fill = terms)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~component, nrow = 1) + 
  labs(y = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, we can see how much every component contributes to each &lt;code&gt;category&lt;/code&gt;. Clearly, the biggest difference in PC1 is powdered sugar vs. simple syrup. Let’s zoom in on the first four components, and understand which cocktail ingredients contribute in the positive and negative directions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)

tidied_pca %&amp;gt;% 
  filter(component %in% paste0(&amp;quot;PC&amp;quot;, 1:4)) %&amp;gt;% 
  group_by(component) %&amp;gt;% 
  top_n(8, abs(value)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(terms = reorder_within(terms, abs(value), component)) %&amp;gt;% 
  ggplot(aes(abs(value), terms, fill = value &amp;gt; 0)) + 
  geom_col() + 
  scale_y_reordered() +
  facet_wrap(~component, scales = &amp;quot;free_y&amp;quot;) + 
  labs(fill = &amp;quot;positive?&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So PC1 is about powdered sugar + egg + gin drinks vs. simple syrup + lime + tequila drinks. This is the component that explains the most variation in drinks. PC2 is mostly about vermouth, both sweet and dry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bake(pca_prep, new_data = NULL) %&amp;gt;%
  ggplot(aes(PC1, PC2, label = name)) +
  geom_point(aes(color = category), alpha = 0.7, size = 2) +
  geom_text(check_overlap = TRUE) +
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows how the cocktails distributed in the plane of the first two components. The &lt;code&gt;bake()&lt;/code&gt; return the computations accroding to the &lt;code&gt;recipe&lt;/code&gt;. We can conclude:
- Fizzy, egg, poweder sugar drinks are to the left.
- Simple syrup, lime, tequila drinks are to the right.
- Vermouth drinks are more to the top. Although many labels are mixed together, they may be distinguishable on other components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uniform-manifold-approximation-and-projection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Uniform manifold approximation and projection&lt;/h3&gt;
&lt;p&gt;Thanks to the benefits of the &lt;code&gt;tidymodels&lt;/code&gt; ecosystem, we can switch out PCA for UMAP, simply by replace &lt;code&gt;step_pca()&lt;/code&gt; by &lt;code&gt;step_umap&lt;/code&gt;. Before that, don’t forget to import package &lt;code&gt;embed&lt;/code&gt;, which provide recipe steps for ways to create embeddings including UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(embed)

umap_rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
  update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
  step_normalize(all_predictors()) %&amp;gt;% 
  step_umap(all_predictors(), num_comp = 5)

umap_prep &amp;lt;- prep(umap_rec)

umap_prep&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, we can see how the cocktails are distributed in the plane of the first two UMAP components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bake(umap_prep, new_data = NULL) %&amp;gt;%
  ggplot(aes(umap_1, umap_2, label = name)) +
  geom_point(aes(color = category), alpha = 0.7, size = 2) +
  geom_text(check_overlap = TRUE) +
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that the distribution of the first two components of UMAP is totally different from PCA. Obviously, most of the categories didn’t mix together. That’s what I’m talking about. And I can tell, the performance of the classification algorithm trained on the first two components of UMAP will definitely better than of PCA. Let’s prove it on the kernel SVM (Support Vector Machine).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(e1071)

svm_test &amp;lt;- function(num_compo, meth){
  if(meth == &amp;quot;PCA&amp;quot;){
    rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
    update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
    step_normalize(all_predictors()) %&amp;gt;% 
    step_pca(all_predictors(), num_comp = num_compo)
  }
  
  if(meth == &amp;quot;UMAP&amp;quot;){
    rec &amp;lt;- recipe(~ ., data = cocktails_df) %&amp;gt;% 
    update_role(name, category, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% 
    step_normalize(all_predictors()) %&amp;gt;% 
    step_umap(all_predictors(), num_comp = num_compo)
  }
  
  df &amp;lt;- bake(prep(rec), new_data = NULL)
  
  n &amp;lt;- nrow(df)
  ntrain &amp;lt;- round(n * 0.75)
  set.seed(123)
  
  tindex &amp;lt;- sample(n, ntrain)
  train_iris &amp;lt;- df[tindex, ]
  test_iris &amp;lt;- df[-tindex, ]

  svm1 &amp;lt;- svm(category~., data = train_iris, method = &amp;quot;C-classification&amp;quot;, 
              kernal=&amp;quot;radial&amp;quot;, gamma = 0.1, cost = 10)
  
  prediction &amp;lt;- predict(svm1, test_iris)

  xtab &amp;lt;- table(test_iris$category, prediction)
  result &amp;lt;- data.frame(test_iris$category, prediction)
  accuracy &amp;lt;- sum(test_iris$category == prediction) / nrow(test_iris)
  return(accuracy)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a function require two variables input, the number of component and demensionality reduction method, and output the accuracy of the classification. Let’s first verify the performance of the first two components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;PCA: &amp;quot;,svm_test(2, &amp;quot;PCA&amp;quot;), &amp;quot;, UMAP: &amp;quot;,svm_test(2,&amp;quot;UMAP&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PCA:  0.6410256 , UMAP:  0.7051282&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, the first two components of UMAP contains more classification information, at least make the SVM classify more accurate.&lt;/p&gt;
&lt;p&gt;You think that’s the end of it? We know that, in real life, we should always consider the time cost, and that is the meaningful of the dimensionality reduction method. So let add the time cost into the comparision, start by finding out how many components PCA needs to achieve the same accuracy as UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result_pca &amp;lt;- 1:10 %&amp;gt;% map_dbl(svm_test, meth = &amp;quot;PCA&amp;quot;) 
result_umap &amp;lt;- 1:10 %&amp;gt;% map_dbl(svm_test, meth = &amp;quot;UMAP&amp;quot;) 
result &amp;lt;- data.frame(pca = result_pca,
                     umap = result_umap) %&amp;gt;% stack()

result$index &amp;lt;- c(1:10,1:10)

result  %&amp;gt;% 
  ggplot(aes(index, values, color = ind)) + 
  geom_line() + 
  labs(color = &amp;quot;method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://yucao16.com/blog/pca-and-umap/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Seems like the accuracy converge at 75% as the number of components increase, actually after 5 components, PCA performs better than UMAP. Basically, PCA needs at least 5 components to get the height of UMAP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
time_UMAP &amp;lt;- microbenchmark(svm_test(3,&amp;quot;UMAP&amp;quot;), times = 10, unit=&amp;quot;s&amp;quot;)
time_PCA &amp;lt;- microbenchmark(svm_test(5,&amp;quot;PCA&amp;quot;), times = 10, unit=&amp;quot;s&amp;quot;)
cat(&amp;quot;UMAP: &amp;quot;, mean(time_UMAP$time)/1e9, &amp;quot;, PCA: &amp;quot;, mean(time_PCA$time/1e9))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## UMAP:  2.005904 , PCA:  0.330938&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the result, PCA undoubtedly crushes UMAP. I know, I know, you mean that this is because the dataset is so small that the advantages of low dimensionality are not fully realezed, and this is just one example and doesn’t tell us much. I totally agree, I really am. &lt;strong&gt;What I would like to say is, the job of a data analyst should not be to blindly pursue new technologies, but to be flexible and efficient in getting tasks done.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Some of the code for the data mining section is taken from &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;here&lt;/a&gt;, I’m very grateful to &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;Julia Silge&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=_1msVvPE_KY&#34;&gt;David Robinson&lt;/a&gt; for their videos.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leave a message if you like this blog.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Yelp Elite System Analysis</title>
      <link>https://yucao16.com/project/yelp-dataset/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/project/yelp-dataset/</guid>
      <description>&lt;h2 id=&#34;research-question-and-aim&#34;&gt;Research Question and Aim&lt;/h2&gt;
&lt;p&gt;The Yelp platform&amp;rsquo;s elite system has swayed our choices in many cases, however whether Yelp&amp;rsquo;s elite system is trustworthy is something that needs to be explored. Therefore, in this report, we analysed the users in the Yelp dataset who have reviewed/rated restaurants in Phoenix to ﬁnd out if the ratings of elite users are close to those of the regular user, and if not, how to select the users who best represent the tastes of the regular user, i.e. those with the least diﬀerence in ratings from the regular user. At the same time, this paper tests whether machine learning algorithms can be used to predict users who truly reflect the preferences of the average user based on existing user information.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In general, there is an 8% chance that the elite users of the Yelp platform will mislead the regular user. It is possible to select a subset of users that are more representative of the majority of users’ tastes through some selection methods, i.e. the ratings of these users are more informative. And it is feasible to use machine learning algorithms to predict this subset of users based on their features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If interested in the details of the report, please click on the PDF block at the top of the page.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Credit Card Default detection.</title>
      <link>https://yucao16.com/project/credit-card-default-detection/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/project/credit-card-default-detection/</guid>
      <description>&lt;h2 id=&#34;research-question-and-aim&#34;&gt;Research Question and Aim&lt;/h2&gt;
&lt;p&gt;The prediction of whether a customer will default on a loan is a long-standing challenge. In this paper, we explore both dealing with missing values in the data and data imbalances, as well as comparing various machine learning methods.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By adding weights to the algorithms we tested the limits of the two machine learning methods. By weighting the methods it is possible to achieve a 100% recall for both algorithms, albeit at the cost of accuracy. But for the prediction of loan defaults, such a trade-off is mostly worthwhile. And of the two algorithms, weighted logistic regression is the more impressive performer, yielded higher recall rate and AUC in both cases.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If interested in the details of the report, please click on the PDF block at the top of the page.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://yucao16.com/about/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://yucao16.com/resume/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/resume/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Application of Generative Adversarial Networks on Unbalanced Datasets</title>
      <link>https://yucao16.com/project/gan-application/</link>
      <pubDate>Tue, 27 Apr 2010 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/project/gan-application/</guid>
      <description>&lt;h2 id=&#34;research-question-and-aim&#34;&gt;Research Question and Aim&lt;/h2&gt;
&lt;p&gt;The plausibility of existing GAN-based (Generative Adversarial Network) oversampling models is questioned through a model remodelling and literature review approach, while the application of GANs to unbalanced datasets is explored, and information security issues when using GANs are considered.&lt;/p&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Class imbalance is a common problem in data that impedes the predictive performance of classification algorithms. In the task of loan default prediction, the impact of the imbalance on classifiers can often be economically costly. Oversampling methods are commonly used to deal with unbalanced datasets, and a large number of linear interpolation and KNN-based oversampling methods such as SMOTE, ADASYN and their variants are constantly proposed by scholars. However, they have an inherent disadvantage in dealing with high-dimensional and complex datasets. Deep learning networks can model complex data well, and models based on generative adversarial networks(GANs) have made relatively significant progress in generating tabular data  (e.g., database tables). Research at this stage has generally focused on the use of GAN for generating tabular data as novel oversampling tools, however this often requires complex structures and extensive hyper-parameter tuning. As an exploration of the application of GAN to unbalanced datasets, this paper proposes a framework that combines GANs with traditional oversampling methods. We compare our framework with five resampling methods and the results demonstrate that it can lead to better stability of the classifier in the presence of insufficient data.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We proposed a framework for handling imbalanced datasets based on CTGAN and SMOTE, CTGANS, and compared it with three oversampling methods, an undersampling method and a balanced dataset. We evaluated the P2P lending dataset with two classification algorithms, using three metrics of classification performance. We first evaluated the performance of the CTGAN model for generating tabular data and the validity of the CTGAN-based oversampling approach. The results showed that CTGAN successfully generated a complex dataset containing both numerical and categorical variables, with the distribution of most of the variables largely consistent with the original data, but mode collapse persisted in some variables. This resulted in the performance of the CTGAN-based oversampling method being significantly inferior compared to traditional oversampling methods.&lt;/p&gt;
&lt;p&gt;While our method performs largely in line with traditional oversampling methods when the amount of data is sufficient, it has a significant performance advantage when the amount of data is insufficient, using only 10% of the samples to bring the performance of the classification algorithm close to that of other oversampling methods.&lt;/p&gt;
&lt;p&gt;However, when comparing the performance of RLR and MLP, we found that the MLP trained on the datasets balanced by oversampling method performed significantly worse than the RLR model on the test set. Our conjecture is that the complexity of the model shifts the focus of learning from the features in the default data to the structure of the data, i.e. the MLP tends to recognise synthetic data in the dataset. Future research could test our conjecture by setting up two test sets, one with real data and one with synthetic data, and by observing the performance of the MLP on both sets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If interested in the details of the report, please click on the PDF block at the top of the page.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chest X-Ray Pneumonia Classification Using ResNet.</title>
      <link>https://yucao16.com/project/example/</link>
      <pubDate>Tue, 27 Apr 2010 00:00:00 +0000</pubDate>
      <guid>https://yucao16.com/project/example/</guid>
      <description>&lt;h2 id=&#34;research-question-and-aim&#34;&gt;Research Question and Aim&lt;/h2&gt;
&lt;p&gt;In this project, my task was to identify the type of pneumonia based on x-ray photographs, where the labels included normal, common pneumonia and Covid-19. And to make the accuracy rate as high as possible.&lt;/p&gt;
&lt;h2 id=&#34;methodology&#34;&gt;Methodology&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data normalization&lt;/strong&gt;: We normalized the image tensors by subtracting the mean and dividing by the standard deviation of pixels across each channel. Normalizing the data prevents the pixel values from any one channel from disproportionately affecting the losses and gradients.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt;: We applied a random transform when loading the images from the training dataset. Specifically, we crop the photos so that they are transformed into square, 24×24 pixel size photos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Residual connections&lt;/strong&gt;: We added the original input back to the output feature map obtained by passing the input through one or more convolutional layers. (We used the ResNet9 architecture.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Batch normalization&lt;/strong&gt;: After each convolutional layer, we added a batch normalization layer, which normalizes the outputs of the previous layer. This is somewhat similar to data normalization, except it&amp;rsquo;s applied to the outputs of a layer, and the mean and standard deviation are learned parameters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning rate scheduling&lt;/strong&gt;: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and we used the &amp;ldquo;One Cycle Learning Rate Policy&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Weight Decay&lt;/strong&gt;:We added weight decay to the optimizer, yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient clipping&lt;/strong&gt;: We also added gradient clippint, which helps limit the values of gradients to a small range to prevent undesirable changes in model parameters due to large gradient values during training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adam optimizer&lt;/strong&gt;: Instead of SGD (stochastic gradient descent), we used the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. There are many other optimizers to choose froma and experiment with.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results-and-discussion&#34;&gt;Results and Discussion&lt;/h2&gt;
&lt;p&gt;After ten minutes of training, the accuracy finally stabilised at around 95%, while no over-fitting occurred. The effect of the dynamic learning rate and the structure of ResNet is remarkable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If interested in the details of the report, please click on the Code block at the top of the page.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
