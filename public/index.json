[{"authors":null,"categories":null,"content":"My name is Cao Yu and I have just completed my postgraduate degree in Computational Finance at UCL. The topic of my dissertation is deep learning on unbalanced datasets datasets. My interests are in data mining and deep learning, making intuitive and beautiful charts to visualise complex data.\nI am currently looking for a job as a data analyst within London, UK, and I am also applying for a PHD related to deep learning.\n  Download my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yucao16.com/author/yu-cao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yu-cao/","section":"authors","summary":"My name is Cao Yu and I have just completed my postgraduate degree in Computational Finance at UCL. The topic of my dissertation is deep learning on unbalanced datasets datasets. My interests are in data mining and deep learning, making intuitive and beautiful charts to visualise complex data.","tags":null,"title":"Yu Cao","type":"authors"},{"authors":[],"categories":[],"content":"  say hello!\n ","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635153920,"objectID":"ba341120eec021e721105e524e4226e4","permalink":"https://yucao16.com/blog/test/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/blog/test/","section":"blog","summary":"  say hello!\n ","tags":[],"title":"Test","type":"blog"},{"authors":[],"categories":[],"content":"   function myFunction(x) { if (x.style.display === \"none\") { x.style.display = \"block\"; } else { x.style.display = \"none\"; } }   button{ background-color: white; color: #f44336; border-radius:12px; outline: 0; transition-duration: 0.4s; border-color:#f44336; } button:hover{ background-color: #f44336; color: white; }  Note: Don’t be fooled by the subtitle that tells you this article is a 1 min read.\nIn this post, I have listed plenty of plots generated by R. You can enjoy these plots as much as you like. If you are interested in how to reproduce these plots, please click on the Code button below the plots. If you are interested in what these plots can tell you, please click on the Explanation button next to Code. I hope you enjoy this gallery!\nCode pumpkins %\u0026gt;% filter(ott \u0026gt; 20, ott \u0026lt; 1e3) %\u0026gt;% ggplot(aes(ott, weight_lbs, color = place)) + geom_point(alpha = 0.2, size = 1.1) + labs(x = \u0026quot;over-the-top inches\u0026quot;, y = \u0026quot;weight (lbs)\u0026quot;) + scale_color_viridis_c()   Code pumpkins %\u0026gt;% filter(ott \u0026gt; 20, ott \u0026lt; 1e3) %\u0026gt;% ggplot(aes(ott, weight_lbs)) + geom_point(alpha = 0.2, size = 1.1, color = \u0026quot;gray60\u0026quot;) + geom_smooth(aes(color = factor(year)), method = lm, formula = y ~ splines::bs(x, 3), se = FALSE, size = 1.5, alpha = 0.6 ) + labs(x = \u0026quot;over-the-top inches\u0026quot;, y = \u0026quot;weight (lbs)\u0026quot;, color = NULL) + scale_color_viridis_d()   Code pumpkins %\u0026gt;% mutate( country = fct_lump(country, n = 10), country = fct_reorder(country, weight_lbs) ) %\u0026gt;% ggplot(aes(country, weight_lbs, color = country)) + geom_boxplot(outlier.colour = NA) + geom_jitter(alpha = 0.1, width = 0.15) + labs(x = NULL, y = \u0026quot;weight (lbs)\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;)  Explanation  dafsafdafdafda\n  Code water_raw %\u0026gt;% filter( country_name == \u0026quot;Sierra Leone\u0026quot;, lat_deg \u0026gt; 0, lat_deg \u0026lt; 15, lon_deg \u0026lt; 0, status_id %in% c(\u0026quot;y\u0026quot;, \u0026quot;n\u0026quot;) ) %\u0026gt;% ggplot(aes(lon_deg, lat_deg, color = status_id)) + geom_point(alpha = 0.1) + coord_fixed() + guides(color = guide_legend(override.aes = list(alpha = 1)))  Explanation    To be continued…\n","date":1635033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635093312,"objectID":"f1d6f585a3c8af809352195f643ab32b","permalink":"https://yucao16.com/blog/test-hide-code/","publishdate":"2021-10-24T00:00:00Z","relpermalink":"/blog/test-hide-code/","section":"blog","summary":"In this post, I have listed plenty of plots generated by R. I hope you enjoy this gallery!","tags":[],"title":"R Plot Gallery","type":"blog"},{"authors":[],"categories":["Data Mining"],"content":"  In this blog, I mainly compared two dimensionality reduction methods, i.e. PCA (Principle Component Analysis) and UMAP (Uniform manifold approximation and projection). Let’s explore which one is better, PCA, the traditional method of dimensionality reduction or, UMAP the new boy.\nLet’s start with data mining. If you want to remember the codes, then you need to practice them everyday, right?\nlibrary(tidyverse) theme_set(theme_light()) library(janitor) The dataset used in this blog comes from TidyTuesday Dataset, 2020-05-26, about cocktail recipes.\nboston_cocktails \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-26/boston_cocktails.csv\u0026quot;) boston_cocktails %\u0026gt;% count(ingredient, sort = TRUE) ## # A tibble: 569 × 2 ## ingredient n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Gin 176 ## 2 Fresh lemon juice 138 ## 3 Simple Syrup 115 ## 4 Vodka 114 ## 5 Light Rum 113 ## 6 Dry Vermouth 107 ## 7 Fresh Lime Juice 107 ## 8 Triple Sec 107 ## 9 Powdered Sugar 90 ## 10 Grenadine 85 ## # … with 559 more rows Although there are no missing values in this dataset, the ‘ingredient’ column, for example, contains many duplicate labes, such as Juice of a Lime and Lime Juice. So I did a series of data cleaning, which involved sorting out the duplicate labels in column ‘ingredient’ and, converting the categorical variables in the ‘measure’ column to numerical varables, and here is the codes:\ncocktails_parsed \u0026lt;- boston_cocktails %\u0026gt;% filter(!measure == \u0026quot;splash\u0026quot;) %\u0026gt;% mutate( ingredient = str_to_lower(ingredient), ingredient = str_replace_all(ingredient, \u0026quot;-\u0026quot;, \u0026quot; \u0026quot;), ingredient = str_remove(ingredient, \u0026quot; liqueur$\u0026quot;), ingredient = str_remove(ingredient, \u0026quot; (if desired)$\u0026quot;), ingredient = case_when( str_detect(ingredient, \u0026quot;bitters\u0026quot;) ~ \u0026quot;bitters\u0026quot;, str_detect(ingredient, \u0026quot;lemon\u0026quot;) ~ \u0026quot;lemon juice\u0026quot;, str_detect(ingredient, \u0026quot;lime\u0026quot;) ~ \u0026quot;lime juice\u0026quot;, str_detect(ingredient, \u0026quot;grapefruit\u0026quot;) ~ \u0026quot;grapefruit juice\u0026quot;, str_detect(ingredient, \u0026quot;orange\u0026quot;) ~ \u0026quot;orange juice\u0026quot;, TRUE ~ ingredient ), measure = str_replace(measure, \u0026quot; ?1/2\u0026quot;, \u0026quot;.5\u0026quot;), measure = str_replace(measure, \u0026quot; ?3/4\u0026quot;, \u0026quot;.75\u0026quot;), measure = str_replace(measure, \u0026quot; ?1/4\u0026quot;, \u0026quot;.25\u0026quot;), measure = str_replace(measure, \u0026quot; ?For glass\u0026quot;, \u0026quot;12\u0026quot;), measure_number = parse_number(measure)) %\u0026gt;% add_count(ingredient) %\u0026gt;% filter(n \u0026gt; 15) %\u0026gt;% select(-n) %\u0026gt;% distinct(row_id, ingredient, .keep_all = TRUE) %\u0026gt;% na.omit() %\u0026gt;% select(-ingredient_number, -row_id, -measure) Now it looks much better, doesn’t it?\ncocktails_parsed ## # A tibble: 2,561 × 4 ## name category ingredient measure_number ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Gauguin Cocktail Classics light rum 2 ## 2 Gauguin Cocktail Classics lemon juice 1 ## 3 Gauguin Cocktail Classics lime juice 1 ## 4 Fort Lauderdale Cocktail Classics light rum 1.5 ## 5 Fort Lauderdale Cocktail Classics sweet vermouth 0.5 ## 6 Fort Lauderdale Cocktail Classics orange juice 0.25 ## 7 Fort Lauderdale Cocktail Classics lime juice 0.25 ## 8 Cuban Cocktail No. 1 Cocktail Classics lime juice 0.5 ## 9 Cuban Cocktail No. 1 Cocktail Classics powdered sugar 0.5 ## 10 Cuban Cocktail No. 1 Cocktail Classics light rum 2 ## # … with 2,551 more rows It’s time for modeling, in order to apply dimensionality reduction method, you should at least have a high dimension dataset, right? What I gonna do is make the dataset wider by using pivot_wider() from package tidyr.\ncocktails_df \u0026lt;- cocktails_parsed %\u0026gt;% pivot_wider(names_from = ingredient, values_from = measure_number, values_fill = 0) %\u0026gt;% janitor::clean_names() %\u0026gt;% na.omit() cocktails_df ## # A tibble: 937 × 42 ## name category light_rum lemon_juice lime_juice sweet_vermouth orange_juice ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Gauguin Cocktai… 2 1 1 0 0 ## 2 Fort L… Cocktai… 1.5 0 0.25 0.5 0.25 ## 3 Cuban … Cocktai… 2 0 0.5 0 0 ## 4 Cool C… Cocktai… 0 0 0 0 1 ## 5 John C… Whiskies 0 1 0 0 0 ## 6 Cherry… Cocktai… 1.25 0 0 0 0 ## 7 Casa B… Cocktai… 2 0 1.5 0 0 ## 8 Caribb… Cocktai… 0.5 0 0 0 0 ## 9 Amber … Cordial… 0 0.25 0 0 0 ## 10 The Jo… Whiskies 0 0.5 0 0 0 ## # … with 927 more rows, and 35 more variables: powdered_sugar \u0026lt;dbl\u0026gt;, ## # dark_rum \u0026lt;dbl\u0026gt;, cranberry_juice \u0026lt;dbl\u0026gt;, pineapple_juice \u0026lt;dbl\u0026gt;, ## # bourbon_whiskey \u0026lt;dbl\u0026gt;, simple_syrup \u0026lt;dbl\u0026gt;, cherry_flavored_brandy \u0026lt;dbl\u0026gt;, ## # light_cream \u0026lt;dbl\u0026gt;, triple_sec \u0026lt;dbl\u0026gt;, maraschino \u0026lt;dbl\u0026gt;, amaretto \u0026lt;dbl\u0026gt;, ## # grenadine \u0026lt;dbl\u0026gt;, apple_brandy \u0026lt;dbl\u0026gt;, brandy \u0026lt;dbl\u0026gt;, gin \u0026lt;dbl\u0026gt;, ## # anisette \u0026lt;dbl\u0026gt;, dry_vermouth \u0026lt;dbl\u0026gt;, apricot_flavored_brandy \u0026lt;dbl\u0026gt;, ## # bitters \u0026lt;dbl\u0026gt;, straight_rye_whiskey \u0026lt;dbl\u0026gt;, benedictine \u0026lt;dbl\u0026gt;, … Here we go, we got a dataset with 42 columns, which is a series problem if you gonna do some classification word on this dataset. In the following, I will demonstrate the two dimensionality reduction method separately.\nPrinciple Component Analysis One of the benefits of the tidymodels ecosystem is the flexibility and ease of trying different approaches for the same kind of task. With function recipe, you don’t have to perform every step, you only need to write them down.\nlibrary(tidymodels) pca_rec \u0026lt;- recipe(~ ., data = cocktails_df) %\u0026gt;% update_role(name, category, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% step_normalize(all_predictors()) %\u0026gt;% step_pca(all_predictors(), num_comp = 2) pca_prep \u0026lt;- prep(pca_rec) pca_prep Here, let’s focus on the variable pca_rec, what we have done is:\n- First, tell recipe() what’s going on with our model and what data we are using. - Second, we update the role of name and category columns by function update_role(), since these are variables we want to keep around as identifiers for rows. - Then we need to center and scale the numeric predictors. - Finally, we use step_pca for the actual principal component analysis. Note that actually, the PCA step is the second step. The prep estimate the required parameters from a training set that can be later applied to other datasets for us.\nOnce we have our recipe done, we can explore the results by using tidy(), which makes it possible to visualize what each components look like.\ntidied_pca \u0026lt;- tidy(pca_prep, 2) tidied_pca %\u0026gt;% filter(component %in% paste0(\u0026quot;PC\u0026quot;, 1:5)) %\u0026gt;% mutate(component = fct_inorder(component)) %\u0026gt;% ggplot(aes(value, terms, fill = terms)) + geom_col(show.legend = FALSE) + facet_wrap(~component, nrow = 1) + labs(y = \u0026quot;\u0026quot;) In the plot above, we can see how much every component contributes to each category. Clearly, the biggest difference in PC1 is powdered sugar vs. simple syrup. Let’s zoom in on the first four components, and understand which cocktail ingredients contribute in the positive and negative directions.\nlibrary(tidytext) tidied_pca %\u0026gt;% filter(component %in% paste0(\u0026quot;PC\u0026quot;, 1:4)) %\u0026gt;% group_by(component) %\u0026gt;% top_n(8, abs(value)) %\u0026gt;% ungroup() %\u0026gt;% mutate(terms = reorder_within(terms, abs(value), component)) %\u0026gt;% ggplot(aes(abs(value), terms, fill = value \u0026gt; 0)) + geom_col() + scale_y_reordered() + facet_wrap(~component, scales = \u0026quot;free_y\u0026quot;) + labs(fill = \u0026quot;positive?\u0026quot;) So PC1 is about powdered sugar + egg + gin drinks vs. simple syrup + lime + tequila drinks. This is the component that explains the most variation in drinks. PC2 is mostly about vermouth, both sweet and dry.\nbake(pca_prep, new_data = NULL) %\u0026gt;% ggplot(aes(PC1, PC2, label = name)) + geom_point(aes(color = category), alpha = 0.7, size = 2) + geom_text(check_overlap = TRUE) + labs(color = NULL) The plot above shows how the cocktails distributed in the plane of the first two components. The bake() return the computations accroding to the recipe. We can conclude: - Fizzy, egg, poweder sugar drinks are to the left. - Simple syrup, lime, tequila drinks are to the right. - Vermouth drinks are more to the top. Although many labels are mixed together, they may be distinguishable on other components.\n Uniform manifold approximation and projection Thanks to the benefits of the tidymodels ecosystem, we can switch out PCA for UMAP, simply by replace step_pca() by step_umap. Before that, don’t forget to import package embed, which provide recipe steps for ways to create embeddings including UMAP.\nlibrary(embed) umap_rec \u0026lt;- recipe(~ ., data = cocktails_df) %\u0026gt;% update_role(name, category, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% step_normalize(all_predictors()) %\u0026gt;% step_umap(all_predictors(), num_comp = 5) umap_prep \u0026lt;- prep(umap_rec) umap_prep Similarly, we can see how the cocktails are distributed in the plane of the first two UMAP components.\nbake(umap_prep, new_data = NULL) %\u0026gt;% ggplot(aes(umap_1, umap_2, label = name)) + geom_point(aes(color = category), alpha = 0.7, size = 2) + geom_text(check_overlap = TRUE) + labs(color = NULL) You can see that the distribution of the first two components of UMAP is totally different from PCA. Obviously, most of the categories didn’t mix together. That’s what I’m talking about. And I can tell, the performance of the classification algorithm trained on the first two components of UMAP will definitely better than of PCA. Let’s prove it on the kernel SVM (Support Vector Machine).\nlibrary(e1071) svm_test \u0026lt;- function(num_compo, meth){ if(meth == \u0026quot;PCA\u0026quot;){ rec \u0026lt;- recipe(~ ., data = cocktails_df) %\u0026gt;% update_role(name, category, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% step_normalize(all_predictors()) %\u0026gt;% step_pca(all_predictors(), num_comp = num_compo) } if(meth == \u0026quot;UMAP\u0026quot;){ rec \u0026lt;- recipe(~ ., data = cocktails_df) %\u0026gt;% update_role(name, category, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% step_normalize(all_predictors()) %\u0026gt;% step_umap(all_predictors(), num_comp = num_compo) } df \u0026lt;- bake(prep(rec), new_data = NULL) n \u0026lt;- nrow(df) ntrain \u0026lt;- round(n * 0.75) set.seed(123) tindex \u0026lt;- sample(n, ntrain) train_iris \u0026lt;- df[tindex, ] test_iris \u0026lt;- df[-tindex, ] svm1 \u0026lt;- svm(category~., data = train_iris, method = \u0026quot;C-classification\u0026quot;, kernal=\u0026quot;radial\u0026quot;, gamma = 0.1, cost = 10) prediction \u0026lt;- predict(svm1, test_iris) xtab \u0026lt;- table(test_iris$category, prediction) result \u0026lt;- data.frame(test_iris$category, prediction) accuracy \u0026lt;- sum(test_iris$category == prediction) / nrow(test_iris) return(accuracy) } Here is a function require two variables input, the number of component and demensionality reduction method, and output the accuracy of the classification. Let’s first verify the performance of the first two components.\ncat(\u0026quot;PCA: \u0026quot;,svm_test(2, \u0026quot;PCA\u0026quot;), \u0026quot;, UMAP: \u0026quot;,svm_test(2,\u0026quot;UMAP\u0026quot;)) ## PCA: 0.6410256 , UMAP: 0.7051282 Obviously, the first two components of UMAP contains more classification information, at least make the SVM classify more accurate.\nYou think that’s the end of it? We know that, in real life, we should always consider the time cost, and that is the meaningful of the dimensionality reduction method. So let add the time cost into the comparision, start by finding out how many components PCA needs to achieve the same accuracy as UMAP.\nresult_pca \u0026lt;- 1:10 %\u0026gt;% map_dbl(svm_test, meth = \u0026quot;PCA\u0026quot;) result_umap \u0026lt;- 1:10 %\u0026gt;% map_dbl(svm_test, meth = \u0026quot;UMAP\u0026quot;) result \u0026lt;- data.frame(pca = result_pca, umap = result_umap) %\u0026gt;% stack() result$index \u0026lt;- c(1:10,1:10) result %\u0026gt;% ggplot(aes(index, values, color = ind)) + geom_line() + labs(color = \u0026quot;method\u0026quot;) Seems like the accuracy converge at 75% as the number of components increase, actually after 5 components, PCA performs better than UMAP. Basically, PCA needs at least 5 components to get the height of UMAP.\nlibrary(microbenchmark) time_UMAP \u0026lt;- microbenchmark(svm_test(3,\u0026quot;UMAP\u0026quot;), times = 10, unit=\u0026quot;s\u0026quot;) time_PCA \u0026lt;- microbenchmark(svm_test(5,\u0026quot;PCA\u0026quot;), times = 10, unit=\u0026quot;s\u0026quot;) cat(\u0026quot;UMAP: \u0026quot;, mean(time_UMAP$time)/1e9, \u0026quot;, PCA: \u0026quot;, mean(time_PCA$time/1e9)) ## UMAP: 2.020654 , PCA: 0.285047 Here is the result, PCA undoubtedly crushes UMAP. I know, I know, you mean that this is because the dataset is so small that the advantages of low dimensionality are not fully realezed, and this is just one example and doesn’t tell us much. I totally agree, I really am. What I would like to say is, the job of a data analyst should not be to blindly pursue new technologies, but to be flexible and efficient in getting tasks done.\nSome of the code for the data mining section is taken from here, I’m very grateful to Julia Silge and David Robinson for their videos.\nLeave a message if you like this blog.\n ","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633603250,"objectID":"2ca4318821d69046341b9cc49de7fedb","permalink":"https://yucao16.com/blog/pca-and-umap/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/blog/pca-and-umap/","section":"blog","summary":"Comparision of the performance of Principle Component Analysis and Uniform manifold approximation and projection","tags":[],"title":"Comparison of data dimensionality reduction methods!","type":"blog"},{"authors":null,"categories":null,"content":"Research Question and Aim The Yelp platform\u0026rsquo;s elite system has swayed our choices in many cases, however whether Yelp\u0026rsquo;s elite system is trustworthy is something that needs to be explored. Therefore, in this report, we analysed the users in the Yelp dataset who have reviewed/rated restaurants in Phoenix to ﬁnd out if the ratings of elite users are close to those of the regular user, and if not, how to select the users who best represent the tastes of the regular user, i.e. those with the least diﬀerence in ratings from the regular user. At the same time, this paper tests whether machine learning algorithms can be used to predict users who truly reflect the preferences of the average user based on existing user information.\nConclusion In general, there is an 8% chance that the elite users of the Yelp platform will mislead the regular user. It is possible to select a subset of users that are more representative of the majority of users’ tastes through some selection methods, i.e. the ratings of these users are more informative. And it is feasible to use machine learning algorithms to predict this subset of users based on their features.\nIf interested in the details of the report, please click on the PDF block at the top of the page.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"f28c6f4b41bd6703c6b8a327bacba11c","permalink":"https://yucao16.com/project/yelp-dataset/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/yelp-dataset/","section":"project","summary":"Analysis of who best represents the regular user, the elite or something else?","tags":["Data Mining","Other"],"title":"Yelp Elite System Analysis","type":"project"},{"authors":null,"categories":null,"content":"Research Question and Aim The prediction of whether a customer will default on a loan is a long-standing challenge. In this paper, we explore both dealing with missing values in the data and data imbalances, as well as comparing various machine learning methods.\nConclusion By adding weights to the algorithms we tested the limits of the two machine learning methods. By weighting the methods it is possible to achieve a 100% recall for both algorithms, albeit at the cost of accuracy. But for the prediction of loan defaults, such a trade-off is mostly worthwhile. And of the two algorithms, weighted logistic regression is the more impressive performer, yielded higher recall rate and AUC in both cases.\nIf interested in the details of the report, please click on the PDF block at the top of the page.\n","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585267200,"objectID":"07f65ba4ee33621247b24ae54eea63ca","permalink":"https://yucao16.com/project/credit-card-default-detection/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/project/credit-card-default-detection/","section":"project","summary":"Build a model to predict whether a customer will default on their loan.","tags":["Deep Learning","Classification Task"],"title":"Credit Card Default detection.","type":"project"},{"authors":null,"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://yucao16.com/about/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About","tags":null,"title":"About","type":"widget_page"},{"authors":null,"categories":null,"content":"Research Question and Aim The plausibility of existing GAN-based (Generative Adversarial Network) oversampling models is questioned through a model remodelling and literature review approach, while the application of GANs to unbalanced datasets is explored, and information security issues when using GANs are considered.\nAbstract Class imbalance is a common problem in data that impedes the predictive performance of classification algorithms. In the task of loan default prediction, the impact of the imbalance on classifiers can often be economically costly. Oversampling methods are commonly used to deal with unbalanced datasets, and a large number of linear interpolation and KNN-based oversampling methods such as SMOTE, ADASYN and their variants are constantly proposed by scholars. However, they have an inherent disadvantage in dealing with high-dimensional and complex datasets. Deep learning networks can model complex data well, and models based on generative adversarial networks(GANs) have made relatively significant progress in generating tabular data (e.g., database tables). Research at this stage has generally focused on the use of GAN for generating tabular data as novel oversampling tools, however this often requires complex structures and extensive hyper-parameter tuning. As an exploration of the application of GAN to unbalanced datasets, this paper proposes a framework that combines GANs with traditional oversampling methods. We compare our framework with five resampling methods and the results demonstrate that it can lead to better stability of the classifier in the presence of insufficient data.\nConclusion We proposed a framework for handling imbalanced datasets based on CTGAN and SMOTE, CTGANS, and compared it with three oversampling methods, an undersampling method and a balanced dataset. We evaluated the P2P lending dataset with two classification algorithms, using three metrics of classification performance. We first evaluated the performance of the CTGAN model for generating tabular data and the validity of the CTGAN-based oversampling approach. The results showed that CTGAN successfully generated a complex dataset containing both numerical and categorical variables, with the distribution of most of the variables largely consistent with the original data, but mode collapse persisted in some variables. This resulted in the performance of the CTGAN-based oversampling method being significantly inferior compared to traditional oversampling methods.\nWhile our method performs largely in line with traditional oversampling methods when the amount of data is sufficient, it has a significant performance advantage when the amount of data is insufficient, using only 10% of the samples to bring the performance of the classification algorithm close to that of other oversampling methods.\nHowever, when comparing the performance of RLR and MLP, we found that the MLP trained on the datasets balanced by oversampling method performed significantly worse than the RLR model on the test set. Our conjecture is that the complexity of the model shifts the focus of learning from the features in the default data to the structure of the data, i.e. the MLP tends to recognise synthetic data in the dataset. Future research could test our conjecture by setting up two test sets, one with real data and one with synthetic data, and by observing the performance of the MLP on both sets.\nIf interested in the details of the report, please click on the PDF block at the top of the page.\n","date":1272326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1272326400,"objectID":"76177ca2d77be82ab8159038778a1866","permalink":"https://yucao16.com/project/gan-application/","publishdate":"2010-04-27T00:00:00Z","relpermalink":"/project/gan-application/","section":"project","summary":"Questioning existing methods and exploring new applications, the right way to apply GANs on unbalanced datasets?","tags":["Deep Learning","Classification Task","Generative Adversarial Networks"],"title":"Application of Generative Adversarial Networks on Unbalanced Datasets","type":"project"},{"authors":null,"categories":null,"content":"Research Question and Aim In this project, my task was to identify the type of pneumonia based on x-ray photographs, where the labels included normal, common pneumonia and Covid-19. And to make the accuracy rate as high as possible.\nMethodology   Data normalization: We normalized the image tensors by subtracting the mean and dividing by the standard deviation of pixels across each channel. Normalizing the data prevents the pixel values from any one channel from disproportionately affecting the losses and gradients.\n  Data augmentation: We applied a random transform when loading the images from the training dataset. Specifically, we crop the photos so that they are transformed into square, 24×24 pixel size photos.\n  Residual connections: We added the original input back to the output feature map obtained by passing the input through one or more convolutional layers. (We used the ResNet9 architecture.)\n  Batch normalization: After each convolutional layer, we added a batch normalization layer, which normalizes the outputs of the previous layer. This is somewhat similar to data normalization, except it\u0026rsquo;s applied to the outputs of a layer, and the mean and standard deviation are learned parameters.\n  Learning rate scheduling: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. There are many strategies for varying the learning rate during training, and we used the \u0026ldquo;One Cycle Learning Rate Policy\u0026rdquo;.\n  Weight Decay:We added weight decay to the optimizer, yet another regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.\n  Gradient clipping: We also added gradient clippint, which helps limit the values of gradients to a small range to prevent undesirable changes in model parameters due to large gradient values during training.\n  Adam optimizer: Instead of SGD (stochastic gradient descent), we used the Adam optimizer which uses techniques like momentum and adaptive learning rates for faster training. There are many other optimizers to choose froma and experiment with.\n  Results and Discussion After ten minutes of training, the accuracy finally stabilised at around 95%, while no over-fitting occurred. The effect of the dynamic learning rate and the structure of ResNet is remarkable.\nIf interested in the details of the report, please click on the Code block at the top of the page.\n","date":1272326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1272326400,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://yucao16.com/project/example/","publishdate":"2010-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"The use of ResNet enabled the model to achieve 95% accuracy in classifying the type of pneumonia based on chest x-ray.","tags":["Deep Learning","Classification Task"],"title":"Chest X-Ray Pneumonia Classification Using ResNet.","type":"project"}]